class_name <- unique(Y)
for(i in 1:k){
test <- sample( size*(i-1)+1: size*i );
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A(train, c(1:NCOL(A))), Y(train))
Ycalc <- classify(A(test, c(1:NCOL(A))), beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y(test))
error <- error/k
}
return(error)
}
K-fold-crossValid <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- sample( size*(i-1)+1: size*i );
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A(train, c(1:NCOL(A))), Y(train))
Ycalc <- classify(A(test, c(1:NCOL(A))), beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y(test))
}
error <- error/k
return(error)
}
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- sample( size*(i-1)+1: size*i );
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A(train, c(1:NCOL(A))), Y(train))
Ycalc <- classify(A(test, c(1:NCOL(A))), beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y(test))
}
error <- error/k
return(error)
}
setwd('/Users/Rushi/Documents/Masters/ML/Project 1')
library('MASS')
data<-read.csv('iris.txt',header=FALSE)
newdata<-data
newdata$V5<-NULL
A<-data.matrix(newdata, rownames.force = NA)
transpose<-t(A)
Y<-matrix(data$V5)
Y<-sapply(data$V5,switch,'Iris-setosa'=1,'Iris-versicolor'=2,'Iris-virginica'=3,'undefined'="")
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
KVError<-(KFCV(A, Y, 10))
KVError<-KFCV(A, Y, 10)
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- sample( size*(i-1)+1: size*i );
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A[train, c(1:NCOL(A))], Y(train))
Ycalc <- classify(A[test, c(1:NCOL(A))], beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y(test))
}
error <- error/k
return(error)
}
setwd('/Users/Rushi/Documents/Masters/ML/Project 1')
library('MASS')
data<-read.csv('iris.txt',header=FALSE)
newdata<-data
newdata$V5<-NULL
A<-data.matrix(newdata, rownames.force = NA)
transpose<-t(A)
Y<-matrix(data$V5)
Y<-sapply(data$V5,switch,'Iris-setosa'=1,'Iris-versicolor'=2,'Iris-virginica'=3,'undefined'="")
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
KVError<-KFCV(A, Y, 10)
1:NCOL(A)
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- sample( size*(i-1)+1: size*i );
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A[train, (1:NCOL(A))], Y(train))
Ycalc <- classify(A[test, (1:NCOL(A))], beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y(test))
}
error <- error/k
return(error)
}
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
KVError<-KFCV(A, Y, 10)
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- sample( size*(i-1)+1: size*i );
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A(train, (1:NCOL(A))), Y(train))
Ycalc <- classify(A(test, (1:NCOL(A))), beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y(test))
}
error <- error/k
return(error)
}
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
KVError<-KFCV(A, Y, 10)
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- sample( size*(i-1)+1: size*i );
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A[train, ], Y(train))
Ycalc <- classify(A[test, ], beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y(test))
}
error <- error/k
return(error)
}
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
KVError<-KFCV(A, Y, 10)
beta<-linear_regression(A,Y)
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
1:NCOL(A)
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
KVError<-KFCV(A, Y, 10)
beta<-linear_regression(A,Y)
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
setwd('/Users/Rushi/Documents/Masters/ML/Project 1')
library('MASS')
data<-read.csv('iris.txt',header=FALSE)
newdata<-data
newdata$V5<-NULL
A<-data.matrix(newdata, rownames.force = NA)
transpose<-t(A)
Y<-matrix(data$V5)
Y<-sapply(data$V5,switch,'Iris-setosa'=1,'Iris-versicolor'=2,'Iris-virginica'=3,'undefined'="")
# beta<-ginv(transpose%*%A)%*%transpose%*%Y
# K=10
# numberOfSets<-length(Y)/K
#
# #randomized<-array(nrow(Y), dim=c(length(Y), 1, 1))
# folds <- rep_len(Ynew, NROW(Y))
# folds <- sample(folds, NROW(Y))
# i=1
# for(myset in 1:K){
#   fold <- which(folds == myset)
#   train <- Y[-folds,]
#   test <- Y[folds,]
# }
# nrFolds <- 10
# folds <- rep_len(1:nrFolds, nrow(A))
# folds <- sample(folds, nrow(A))
# for(k in 1:nrFolds) {
#   # actual split of the data
#     fold <- which(folds == k)
#     data.train <- A[-fold,]
#     data.test <- A[fold,]
#     }
# classify<-test*beta
#
#
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
KVError<-KFCV(A, Y, 10)
print(paste("Average Kfold Error : ", KVError))
error = 0;
N = 1000;
k = 30;
for (i in 1:N){
error = error + KFCV(A, Y, k);
}
avg = error / N;
print(paste(avg))
#K-fold Cross Validation Function to find error rate
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- sample( size*(i-1)+1: size*i );
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A[train, ], Y(train))
Ycalc <- classify(A[test, ], beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y(test))
}
error <- error/k
return(error)
}
#Classification function
classify <- function(test, beta, length_class){
#No round -
#Yop = test*beta
Yop = round(test*beta)
Yop( Yop > length_class ) = length_class
Yop(Yop < 1) = 1
return(Yop)
}
#Linear Regression Function
linear_regression <- function(A, Y){
beta<-ginv(transpose%*%A)%*%transpose%*%Y
return(beta)
}
#sum of squared error function
sum_of_squared_error = function(Ycalc, Yact){
error = Ycalc - Yact;
error = sum(error * error)/length(Ycalc);
return(error)
}
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
A
Y
Y
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
ginv(transpose%*%A)%*%transpose%*%Y
ginv(transpose%*%A)
A
t(A)
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
source('~/Documents/Masters/ML/Project 1/linearRegression.R')
ginv(transpose*A)A
A
test
A[test, ]
Ycalc <- classify(A[test, ], beta, length(class_name))
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
A[test, ]
test
help sample
sample(1:10)
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
View(Ycalc)
View(Ycalc)
Ycalc <- classify(A[test, ], beta, length(class_name))
View(Ycalc)
A[test, ]%*%beta
beta
A[test, ]
test[38]
randindex
i
size
size*(i-1)
size*(i-1)+1
[size*(i-1)+1:size*i]
randindex[size*(i-1)+1:size*i]
randindex[149:150]
randindex[75:150]
randindex[(size*(i-1)+1):(size*i)]
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
debugSource('~/Documents/Masters/ML/Project 1/linearRegression.R')
source('~/Documents/Masters/ML/Project 1/linearRegression.R')
setwd('/Users/Rushi/Documents/Masters/ML/Project 1')
library('MASS')
data<-read.csv('iris.txt',header=FALSE)
newdata<-data
newdata$V5<-NULL
A<-data.matrix(newdata, rownames.force = NA)
transpose<-t(A)
Y<-matrix(data$V5)
Y<-sapply(data$V5,switch,'Iris-setosa'=1,'Iris-versicolor'=2,'Iris-virginica'=3,'undefined'="")
#K-fold Cross Validation Function to find error rate
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- randindex[ (size*(i-1)+1): (size*i) ];
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A[train, ], Y[train])
Ycalc <- classify(A[test, ], beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y[test])
}
error <- error/k
return(error)
}
#Classification function
classify <- function(test, beta, length_class){
#No round -
#Yop = test*beta
Yop <- round(test%*%beta)
Yop[ Yop > length_class ] <- length_class
Yop[Yop < 1] <- 1
return(Yop)
}
#Linear Regression Function
linear_regression <- function(A, Y){
transpose<-t(A)
beta<-ginv(transpose%*%A)%*%transpose%*%Y
return(beta)
}
#sum of squared error function
sum_of_squared_error = function(Ycalc, Yact){
error = Ycalc - Yact;
error = sum(error * error)/length(Ycalc);
return(error)
}
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
k<-10;
KVError<-KFCV(A, Y, k)
print(paste("Average Kfold Error : ", KVError))
error <- 0;
N <- 1000;
for (i in 1:N){
error = error + KFCV(A, Y, k);
}
avg = error / N;
print(paste(avg))
setwd('/Users/Rushi/Documents/Masters/ML/Project 1')
library('MASS')
data<-read.csv('iris.txt',header=FALSE)
newdata<-data
newdata$V5<-NULL
A<-data.matrix(newdata, rownames.force = NA)
transpose<-t(A)
Y<-matrix(data$V5)
Y<-sapply(data$V5,switch,'Iris-setosa'=1,'Iris-versicolor'=2,'Iris-virginica'=3,'undefined'="")
#K-fold Cross Validation Function to find error rate
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- randindex[ (size*(i-1)+1): (size*i) ];
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A[train, ], Y[train])
Ycalc <- classify(A[test, ], beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y[test])
}
error <- error/k
return(error)
}
#Classification function
classify <- function(test, beta, length_class){
#No round -
#Yop = test*beta
Yop <- round(test%*%beta)
Yop[ Yop > length_class ] <- length_class
Yop[Yop < 1] <- 1
return(Yop)
}
#Linear Regression Function
linear_regression <- function(A, Y){
transpose<-t(A)
beta<-ginv(transpose%*%A)%*%transpose%*%Y
return(beta)
}
#sum of squared error function
sum_of_squared_error = function(Ycalc, Yact){
error = Ycalc - Yact;
error = sum(error * error)/length(Ycalc);
return(error)
}
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
k<-30;
KVError<-KFCV(A, Y, k)
print(paste("Average Kfold Error : ", KVError))
error <- 0;
N <- 1000;
for (i in 1:N){
error = error + KFCV(A, Y, k);
}
avg = error / N;
print(paste(avg))
source('~/Documents/Masters/ML/Project 1/linearRegression.R', echo=TRUE)
source('~/Documents/Masters/ML/Project 1/linearRegression.R', echo=TRUE)
source('~/Documents/Masters/ML/Project 1/linearRegression.R', echo=TRUE)
setwd('/Users/Rushi/Documents/Masters/ML/Project 1')
library('MASS')
data<-read.csv('iris.txt',header=FALSE)
newdata<-data
newdata$V5<-NULL
A<-data.matrix(newdata, rownames.force = NA)
transpose<-t(A)
Y<-matrix(data$V5)
Y<-sapply(data$V5,switch,'Iris-setosa'=1,'Iris-versicolor'=2,'Iris-virginica'=3,'undefined'="")
#K-fold Cross Validation Function to find error rate
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- randindex[ (size*(i-1)+1): (size*i) ];
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A[train, ], Y[train])
Ycalc <- classify(A[test, ], beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y[test])
}
error <- error/k
return(error)
}
#Classification function
classify <- function(test, beta, length_class){
#No round -
#Yop = test*beta
Yop <- round(test%*%beta)
Yop[ Yop > length_class ] <- length_class
Yop[Yop < 1] <- 1
return(Yop)
}
#Linear Regression Function
linear_regression <- function(A, Y){
transpose<-t(A)
beta<-ginv(transpose%*%A)%*%transpose%*%Y
return(beta)
}
#sum of squared error function
sum_of_squared_error = function(Ycalc, Yact){
error = Ycalc - Yact;
error = sum(error * error)/length(Ycalc);
return(error)
}
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
k<-50;
KVError<-KFCV(A, Y, k)
print(paste("Average Kfold Error : ", KVError))
error <- 0;
N <- 1000;
for (i in 1:N){
error = error + KFCV(A, Y, k);
}
avg = error / N;
print(paste(avg))
source('~/Documents/Masters/ML/Project 1/linearRegression.R')
setwd('/Users/Rushi/Documents/Masters/ML/Project 1')
library('MASS')
data<-read.csv('iris.txt',header=FALSE)
newdata<-data
newdata$V5<-NULL
A<-data.matrix(newdata, rownames.force = NA)
transpose<-t(A)
Y<-matrix(data$V5)
Y<-sapply(data$V5,switch,'Iris-setosa'=1,'Iris-versicolor'=2,'Iris-virginica'=3,'undefined'="")
#K-fold Cross Validation Function to find error rate
KFCV <- function(A,Y,k){
randindex<-sample(length(Y))
size<-length(Y)/k
error <- 0
class_name <- unique(Y)
for(i in 1:k){
test <- randindex[ (size*(i-1)+1): (size*i) ];
train <- setdiff( 1:length(Y),  test);
beta <- linear_regression(A[train, ], Y[train])
Ycalc <- classify(A[test, ], beta, length(class_name))
error <- error + sum_of_squared_error(Ycalc, Y[test])
}
error <- error/k
return(error)
}
#Classification function
classify <- function(test, beta, length_class){
#No round -
#Yop = test*beta
Yop <- round(test%*%beta)
Yop[ Yop > length_class ] <- length_class
Yop[Yop < 1] <- 1
return(Yop)
}
#Linear Regression Function
linear_regression <- function(A, Y){
transpose<-t(A)
beta<-ginv(transpose%*%A)%*%transpose%*%Y
return(beta)
}
#sum of squared error function
sum_of_squared_error = function(Ycalc, Yact){
error = Ycalc - Yact;
error = sum(error * error)/length(Ycalc);
return(error)
}
beta<-linear_regression(A,Y)
print(paste("Beta :", beta))
k<-10;
KVError<-KFCV(A, Y, k)
error <- 0;
N <- 1000;
for (i in 1:N){
error = error + KFCV(A, Y, k);
}
avg = error / N;
print(paste("Average Kfold Error : ", KVError))
print(paste(avg))
source('~/Documents/Masters/ML/Project 1/linearRegression.R')
